{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Topology\n",
    "\n",
    "- Topologies define the available hardware of a datacenter\n",
    "- Defined using JSON format\n",
    "\n",
    "A topology based on surfsara LISA cluster ([file](topologies/surfsara.json)):\n",
    "```json\n",
    "{\n",
    "    \"clusters\":\n",
    "    [\n",
    "        {\n",
    "            \"name\": \"C01\",\n",
    "            \"hosts\" :\n",
    "            [\n",
    "                {\n",
    "                    \"name\": \"H01\",\n",
    "                    \"cpu\":\n",
    "                    {\n",
    "                        \"coreCount\": 16,\n",
    "                        \"coreSpeed\": 2100\n",
    "                    },\n",
    "                    \"memory\": {\n",
    "                        \"memorySize\": 100000\n",
    "                    },\n",
    "                    \"count\": 279\n",
    "                }\n",
    "            ],\n",
    "            \"powerSource\": {\n",
    "                \"carbonTracePath\": \"carbon_traces/carbon_2022.parquet\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Workloads\n",
    "\n",
    "- Workload traces are a recording of the tasks executed on a system \n",
    "\n",
    "- Graph Greenifier requires two different traces:\n",
    "    - **tasks.parquet** provides a general overview of the tasks executed during the workload.\n",
    "    - **fragments.parquet** provides detailed information of each task during its runtime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-12T14:03:43.003208Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tasks = pd.read_parquet(\"workload_traces/2022-10-01_2022-10-31/tasks.parquet\")\n",
    "df_fragments = pd.read_parquet(\"workload_traces/2022-10-01_2022-10-31/fragments.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload Traces\n",
    "\n",
    "- Traces define which tasks should be executed and when\n",
    "\n",
    "- Provided as bag-of-task\n",
    "\n",
    "- In this demo we run 1 month of Surfsara Lisa -> 68,648 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submission_time</th>\n",
       "      <th>duration</th>\n",
       "      <th>cpu_count</th>\n",
       "      <th>cpu_capacity</th>\n",
       "      <th>mem_capacity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2081278</td>\n",
       "      <td>2022-09-30 22:00:00</td>\n",
       "      <td>1194000</td>\n",
       "      <td>16</td>\n",
       "      <td>33600.0</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2081285</td>\n",
       "      <td>2022-09-30 22:00:00</td>\n",
       "      <td>7176000</td>\n",
       "      <td>16</td>\n",
       "      <td>33600.0</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2081310</td>\n",
       "      <td>2022-09-30 22:00:00</td>\n",
       "      <td>17218000</td>\n",
       "      <td>16</td>\n",
       "      <td>33600.0</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2081317</td>\n",
       "      <td>2022-09-30 22:00:00</td>\n",
       "      <td>17605000</td>\n",
       "      <td>16</td>\n",
       "      <td>33600.0</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2081324</td>\n",
       "      <td>2022-09-30 22:00:00</td>\n",
       "      <td>19007000</td>\n",
       "      <td>16</td>\n",
       "      <td>33600.0</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     submission_time  duration  cpu_count  cpu_capacity  \\\n",
       "0  2081278 2022-09-30 22:00:00   1194000         16       33600.0   \n",
       "1  2081285 2022-09-30 22:00:00   7176000         16       33600.0   \n",
       "2  2081310 2022-09-30 22:00:00  17218000         16       33600.0   \n",
       "3  2081317 2022-09-30 22:00:00  17605000         16       33600.0   \n",
       "4  2081324 2022-09-30 22:00:00  19007000         16       33600.0   \n",
       "\n",
       "   mem_capacity  \n",
       "0        100000  \n",
       "1        100000  \n",
       "2        100000  \n",
       "3        100000  \n",
       "4        100000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tasks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fragments\n",
    "\n",
    "<img src=\"./figures/fragments.jpg\" width=600, alt=\"Alternative text\" />\n",
    "\n",
    "##### One month of Surfsara tasks has 10,219,665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>duration</th>\n",
       "      <th>cpu_count</th>\n",
       "      <th>cpu_usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2081278</td>\n",
       "      <td>30000</td>\n",
       "      <td>16</td>\n",
       "      <td>14721.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2081278</td>\n",
       "      <td>30000</td>\n",
       "      <td>16</td>\n",
       "      <td>14700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2081278</td>\n",
       "      <td>30000</td>\n",
       "      <td>16</td>\n",
       "      <td>14700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2081278</td>\n",
       "      <td>30000</td>\n",
       "      <td>16</td>\n",
       "      <td>14700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2081278</td>\n",
       "      <td>30000</td>\n",
       "      <td>16</td>\n",
       "      <td>14868.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  duration  cpu_count  cpu_usage\n",
       "0  2081278     30000         16    14721.0\n",
       "1  2081278     30000         16    14700.0\n",
       "2  2081278     30000         16    14700.0\n",
       "3  2081278     30000         16    14700.0\n",
       "4  2081278     30000         16    14868.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fragments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carbon Trace\n",
    "\n",
    "Carbon Traces define the Carbon Intenisty of the available energy over time\n",
    "\n",
    "Gathered using ENTSO-E\n",
    "\n",
    "Specific to the location of the data center\n",
    "\n",
    "Defined as a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>carbon_intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-31 23:00:00</td>\n",
       "      <td>168.138693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-31 23:15:00</td>\n",
       "      <td>167.050014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-31 23:30:00</td>\n",
       "      <td>164.552936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-31 23:45:00</td>\n",
       "      <td>167.493769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>164.517793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  carbon_intensity\n",
       "0 2021-12-31 23:00:00        168.138693\n",
       "1 2021-12-31 23:15:00        167.050014\n",
       "2 2021-12-31 23:30:00        164.552936\n",
       "3 2021-12-31 23:45:00        167.493769\n",
       "4 2022-01-01 00:00:00        164.517793"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_carbon = pd.read_parquet(\"carbon_traces/carbon_2022.parquet\")\n",
    "\n",
    "df_carbon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "- An Experiment defines what the Graph Greenifier should run, and how.\n",
    "\n",
    "- Eperiments are defined using a JSON format\n",
    "\n",
    "Example Experiment ([file](experiments/surfsara_month.json)):\n",
    "```json\n",
    "{\n",
    "    \"name\": \"surfsara_month\",\n",
    "    \"topologies\": [{\n",
    "        \"pathToFile\": \"topologies/surfsara.json\"\n",
    "    }],\n",
    "    \"workloads\": [{\n",
    "        \"pathToFile\": \"workload_traces/2022-10-01_2022-10-31\",\n",
    "        \"type\": \"ComputeWorkload\"\n",
    "        }],\n",
    "    \"allocationPolicies\": [{\n",
    "        \"type\": \"prefab\",\n",
    "        \"policyName\": \"Mem\"\n",
    "    }],\n",
    "    \"exportModels\": [\n",
    "        {\n",
    "            \"exportInterval\": 3600,\n",
    "            \"computeExportConfig\": {\n",
    "                \"hostExportColumns\": [\"power_draw\", \"energy_usage\", \"cpu_usage\", \"cpu_utilization\"],\n",
    "                \"taskExportColumns\": [\"submission_time\", \"schedule_time\", \"finish_time\", \"task_state\"],\n",
    "                \"serviceExportColumns\": [\"tasks_total\", \"tasks_pending\", \"tasks_active\", \"tasks_completed\", \"tasks_terminated\", \"hosts_up\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Note:** Parameters are defined as lists, this makes it easy to run multiple experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running OpenDC\n",
    "\n",
    "- OpenDC is executed directly from the terminal\n",
    "\n",
    "- Only an experiment file is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "pathToScenario = \"experiments/surfsara_month.json\"\n",
    "result = subprocess.run([\"OpenDCExperimentRunner/bin/OpenDCExperimentRunner.bat\", \"--experiment-path\", pathToScenario])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not open Parquet input source '<Buffer>': Parquet file size is 4 bytes, smaller than the minimum file footer (8 bytes)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df_host \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput/surfsara_month/raw-output/0/seed=0/host.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m df_powerSource \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/surfsara_month/raw-output/0/seed=0/powerSource.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m df_task \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/surfsara_month/raw-output/0/seed=0/task.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    668\u001b[0m     path,\n\u001b[0;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    676\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parquet.py:274\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    268\u001b[0m     path,\n\u001b[0;32m    269\u001b[0m     filesystem,\n\u001b[0;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    275\u001b[0m         path_or_handle,\n\u001b[0;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    277\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    278\u001b[0m         filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    280\u001b[0m     )\n\u001b[0;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyarrow\\parquet\\core.py:1793\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[0;32m   1787\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1788\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated as of pyarrow 15.0.0 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1793\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1794\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyarrow\\parquet\\core.py:1360\u001b[0m, in \u001b[0;36mParquetDataset.__init__\u001b[1;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1357\u001b[0m     fragment \u001b[38;5;241m=\u001b[39m parquet_format\u001b[38;5;241m.\u001b[39mmake_fragment(single_file, filesystem)\n\u001b[0;32m   1359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mFileSystemDataset(\n\u001b[1;32m-> 1360\u001b[0m         [fragment], schema\u001b[38;5;241m=\u001b[39mschema \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mfragment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphysical_schema\u001b[49m,\n\u001b[0;32m   1361\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mparquet_format,\n\u001b[0;32m   1362\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfragment\u001b[38;5;241m.\u001b[39mfilesystem\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;66;03m# check partitioning to enable dictionary encoding\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyarrow\\_dataset.pyx:1443\u001b[0m, in \u001b[0;36mpyarrow._dataset.Fragment.physical_schema.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Could not open Parquet input source '<Buffer>': Parquet file size is 4 bytes, smaller than the minimum file footer (8 bytes)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df_host = pd.read_parquet(\"output/surfsara_month/raw-output/0/seed=0/host.parquet\")\n",
    "df_powerSource = pd.read_parquet(\"output/surfsara_month/raw-output/0/seed=0/powerSource.parquet\")\n",
    "df_task = pd.read_parquet(\"output/surfsara_month/raw-output/0/seed=0/task.parquet\")\n",
    "df_service = pd.read_parquet(\"output/surfsara_month/raw-output/0/seed=0/service.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host\n",
    "- Information about the host at each timestamp. \n",
    "- Examples of metrics: \n",
    "    - cpu_utilization\n",
    "    - power_draw \n",
    "    - energy_usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(f\"The host file contains the following columns:\\n {np.array(df_host.columns)}\\n\")\n",
    "print(f\"The host file consist of {len(df_host)} samples\\n\")\n",
    "df_host.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "- The task file contains all information about the different tasks at each timestamp. \n",
    "- Example use cases:\n",
    "    - when is a task run\n",
    "    - How long did it take\n",
    "    - on which host was a task executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(f\"The task file contains the following columns:\\n {np.array(df_task.columns)}\\n\")\n",
    "print(f\"The task file consist of {len(df_task)} samples\\n\")\n",
    "df_task.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PowerSource\n",
    "- The task file contains all information about the power sources at each timestamp. \n",
    "- Example use cases:\n",
    "    - What is the total energy used during the workload?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(f\"The task file contains the following columns:\\n {np.array(df_powerSource.columns)}\\n\")\n",
    "print(f\"The power file consist of {len(df_powerSource)} samples\\n\")\n",
    "df_powerSource.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service\n",
    "\n",
    "- The service file contains genaral information about the experiments. \n",
    "- Example uses:\n",
    "    - How many tasks are running?\n",
    "    - How many hosts are up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(f\"The service file contains the following columns:\\n {np.array(df_service.columns)}\\n\")\n",
    "print(f\"The service file consist of {len(df_service)} samples\\n\")\n",
    "df_service.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance\n",
    "\n",
    "- To properly compare the different experiments, we would like to aggregate them into meaningfull values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "runtime = pd.to_timedelta(df_service.timestamp.max() - df_service.timestamp.min(), unit=\"ms\")\n",
    "utilization = df_host.cpu_utilization.mean()\n",
    "\n",
    "\n",
    "print(f\"The small data center had a total runtime of {runtime}\")\n",
    "print(f\"On average, the utilization of each host is {utilization * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "energy = df_powerSource.energy_usage.sum() / 3_600_000 # convert energy to kWh\n",
    "carbon = df_powerSource.carbon_emission.sum() / 1000\n",
    "\n",
    "\n",
    "print(f\"The small data center used {energy:.2f} kWh during the workload\")\n",
    "print(f\"The small data center used {carbon:.2f} kg CO2 during the workload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "timestamps = pd.to_datetime(df_powerSource[:-1].timestamp_absolute, unit=\"ms\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(timestamps, df_powerSource[:-1].carbon_emission/1000)\n",
    "\n",
    "plt.title(\"Carbon emission during a month long surfsara workload\", fontsize=15)\n",
    "plt.xlabel(\"Date (yy-mm-dd)\", fontsize=13)\n",
    "plt.ylabel(\"Carbon Emission (kgCO2/h)\", fontsize=13)\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "myFmt = mdates.DateFormatter('%y-%m-%d')\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy was verified in the FootPrinter Paper\n",
    "\n",
    "- One week of Surfsara trace\n",
    "\n",
    "- Sampled every 30 seconds\n",
    "\n",
    "- MAPE of 3.15%\n",
    "\n",
    "<img src=\"./figures/validation.jpg\" width=600, alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footprinter was also used to compare the same workload in different locations\n",
    "\n",
    "<img src=\"./figures/location_comparison.jpg\" width=600, alt=\"Alternative text\" />\n",
    "\n",
    "#### The Location of a data center has a significant effect on the Carbon Emission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failures\n",
    "\n",
    "- Machines can fail during operation\n",
    "\n",
    "- OpenDC can simulate host failures \n",
    "    - Failure Traces\n",
    "    - Failure Models\n",
    "\n",
    "To use failures, we have to update the experiment file:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"name\": \"surfsara_day_failures\",\n",
    "    \"topologies\": [{\n",
    "        \"pathToFile\": \"topologies/surfsara_small.json\"\n",
    "    }],\n",
    "    \"workloads\": [{\n",
    "        \"pathToFile\": \"workload_traces/2022-10-01_2022-10-31\",\n",
    "        \"type\": \"ComputeWorkload\"\n",
    "        }],\n",
    "    \"allocationPolicies\": [{\n",
    "        \"type\": \"prefab\",\n",
    "        \"policyName\": \"Mem\"\n",
    "    }],\n",
    "    \"exportModels\": [\n",
    "        {\n",
    "            \"exportInterval\": 3600,\n",
    "            \"computeExportConfig\": {\n",
    "                \"hostExportColumns\": [\"power_draw\", \"energy_usage\", \"cpu_usage\", \"cpu_utilization\"],\n",
    "                \"taskExportColumns\": [\"submission_time\", \"schedule_time\", \"finish_time\", \"task_state\"],\n",
    "                \"serviceExportColumns\": [\"tasks_total\", \"tasks_pending\", \"tasks_active\", \"tasks_completed\", \"tasks_terminated\", \"hosts_up\"]\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"failureModels\": [\n",
    "        {\n",
    "        \"type\": \"trace-based\",\n",
    "        \"pathToFile\": \"failure_traces/Whatsapp_user_reported.parquet\"\n",
    "    }],\n",
    "    \"maxNumFailures\": [10]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['OpenDCExperimentRunner/bin/OpenDCExperimentRunner', '--experiment-path', 'experiments/surfsara_month_failures.json'], returncode=1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "pathToScenario = \"experiments/surfsara_month_failures.json\"\n",
    "subprocess.run([\"OpenDCExperimentRunner/bin/OpenDCExperimentRunner\", \"--experiment-path\", pathToScenario], shell = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df_host_failures = pd.read_parquet(\"output/surfsara_month_failures/raw-output/0/seed=0/host.parquet\")\n",
    "df_powerSource_failures = pd.read_parquet(\"output/surfsara_month_failures/raw-output/0/seed=0/powerSource.parquet\")\n",
    "df_task_failures = pd.read_parquet(\"output/surfsara_month_failures/raw-output/0/seed=0/task.parquet\")\n",
    "df_service_failures = pd.read_parquet(\"output/surfsara_month_failures/raw-output/0/seed=0/service.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "timestamps = pd.to_datetime(df_service[:-1].timestamp_absolute, unit=\"ms\")\n",
    "timestamps_failures = pd.to_datetime(df_service_failures[:-1].timestamp_absolute, unit=\"ms\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(timestamps, df_service[:-1].tasks_active, label=\"no failures\")\n",
    "ax.plot(timestamps_failures, df_service_failures[:-1].tasks_active, label=\"failures\")\n",
    "\n",
    "plt.title(\"Tasks active during a workload\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Carbon Emission (CO2/h)\")\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "myFmt = mdates.DateFormatter('%y-%m-%d %H:%M:%S')\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "timestamps = pd.to_datetime(df_powerSource[:-1].timestamp_absolute, unit=\"ms\")\n",
    "timestamps_failures = pd.to_datetime(df_powerSource_failures[:-1].timestamp_absolute, unit=\"ms\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(timestamps, df_powerSource[:-1].energy_usage, label=\"no failures\", linewidth=2)\n",
    "ax.plot(timestamps_failures, df_powerSource_failures[:-1].energy_usage, label=\"failures\", linewidth=2)\n",
    "\n",
    "plt.title(\"Energy Usage during a workload (with and without failures)\", fontsize=15)\n",
    "plt.xlabel(\"Time\", fontsize=15)\n",
    "plt.ylabel(\"Energy Usage (kWh)\", fontsize=15)\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "myFmt = mdates.DateFormatter('%y-%m-%d %H:%M:%S')\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "runtime_normal = pd.to_timedelta(df_service.timestamp.max() - df_service.timestamp.min(), unit=\"ms\")\n",
    "runtime_failures = pd.to_timedelta(df_service_failures.timestamp.max() - df_service_failures.timestamp.min(), unit=\"ms\")\n",
    "\n",
    "print(\"RUNTIME\")\n",
    "print(f\"The workload without failures took:   {runtime_normal}\")\n",
    "print(f\"The workload with failures took:      {runtime_failures}\")\n",
    "print(f\"This is an increase of                {(runtime_failures - runtime_normal) / runtime_normal * 100:.2f}%\\n\\n\")\n",
    "\n",
    "energy_normal = df_powerSource.energy_usage.sum()\n",
    "energy_failures = df_powerSource_failures.energy_usage.sum()\n",
    "\n",
    "print(\"ENERGY\")\n",
    "print(f\"The workload without failures used    {energy_normal / 3_600_000:.0f}  kWh\")\n",
    "print(f\"The workload with failures used       {energy_failures / 3_600_000:.0f} kWh\")\n",
    "print(f\"This is an increase of                {(energy_failures - energy_normal) / energy_normal * 100:.2f} %\\n\\n\")\n",
    "\n",
    "carbon_normal = df_powerSource.carbon_emission.sum()\n",
    "carbon_failures = df_powerSource_failures.carbon_emission.sum()\n",
    "\n",
    "print(\"CARBON\")\n",
    "print(f\"The workload without failures emitted {carbon_normal:.0f} grams of carbon\")\n",
    "print(f\"The workload with failures emitted    {carbon_failures:.0f} grams of carbon\")\n",
    "print(f\"This is an increase of                {(carbon_failures - carbon_normal) / carbon_normal * 100:.2f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing\n",
    "\n",
    "- Tasks need to fully rerun on failure\n",
    "\n",
    "- OpenDC can simulate Task checkpointing \n",
    "    1. Tasks take periodic snapshots of their state\n",
    "    2. Tasks can be restarted at the snapshot\n",
    "\n",
    "To use Checkpointing, we have to update the experiment file:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"name\": \"surfsara_month_checkpoint\",\n",
    "    \"runs\": 1,\n",
    "    \"topologies\": [{\n",
    "        \"pathToFile\": \"topologies/surfsara.json\"\n",
    "    }],\n",
    "    \"workloads\": [{\n",
    "        \"pathToFile\": \"workload_traces/2022-10-01_2022-10-31\",\n",
    "        \"type\": \"ComputeWorkload\"\n",
    "        }],\n",
    "    \"allocationPolicies\": [{\n",
    "        \"type\": \"prefab\",\n",
    "        \"policyName\": \"Mem\"\n",
    "    }],\n",
    "    \"exportModels\": [\n",
    "        {\n",
    "            \"exportInterval\": 3600,\n",
    "            \"computeExportConfig\": {\n",
    "                \"hostExportColumns\": [\"power_draw\", \"energy_usage\", \"cpu_usage\", \"cpu_utilization\"],\n",
    "                \"taskExportColumns\": [\"submission_time\", \"schedule_time\", \"finish_time\", \"task_state\"],\n",
    "                \"serviceExportColumns\": [\"tasks_total\", \"tasks_pending\", \"tasks_active\", \"tasks_completed\", \"tasks_terminated\", \"hosts_up\"]\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"failureModels\": [\n",
    "        {\n",
    "        \"type\": \"trace-based\",\n",
    "        \"pathToFile\": \"failure_traces/Whatsapp_user_reported.parquet\"\n",
    "    }],\n",
    "    \"checkpointModels\": [\n",
    "        {\n",
    "            \"checkpointInterval\": 600000,\n",
    "            \"checkpointDuration\": 60000,\n",
    "            \"checkpointIntervalScaling\": 1.0\n",
    "        }\n",
    "    ]\n",
    "}    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "pathToScenario = \"experiments/surfsara_month_checkpoint.json\"\n",
    "subprocess.run([\"OpenDCExperimentRunner/bin/OpenDCExperimentRunner\", \"--experiment-path\", pathToScenario])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df_host_checkpoint = pd.read_parquet(\"output/surfsara_month_checkpoint/raw-output/0/seed=0/host.parquet\")\n",
    "df_powerSource_checkpoint = pd.read_parquet(\"output/surfsara_month_checkpoint/raw-output/0/seed=0/powerSource.parquet\")\n",
    "df_task_checkpoint = pd.read_parquet(\"output/surfsara_month_checkpoint/raw-output/0/seed=0/task.parquet\")\n",
    "df_service_checkpoint = pd.read_parquet(\"output/surfsara_month_checkpoint/raw-output/0/seed=0/service.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "timestamps = pd.to_datetime(df_service[:-1].timestamp_absolute, unit=\"ms\")\n",
    "timestamps_failures = pd.to_datetime(df_service_failures[:-1].timestamp_absolute, unit=\"ms\")\n",
    "timestamps_checkpoint = pd.to_datetime(df_service_checkpoint[:-1].timestamp_absolute, unit=\"ms\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(timestamps, df_service[:-1].tasks_active, label=\"no failures\")\n",
    "ax.plot(timestamps_failures, df_service_failures[:-1].tasks_active, label=\"failures\")\n",
    "ax.plot(timestamps_checkpoint, df_service_checkpoint[:-1].tasks_active, label=\"checkpoint\")\n",
    "\n",
    "plt.title(\"Tasks active during a workload\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Carbon Emission (CO2/h)\")\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "myFmt = mdates.DateFormatter('%y-%m-%d %H:%M:%S')\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "timestamps = pd.to_datetime(df_powerSource[:-1].timestamp_absolute, unit=\"ms\")\n",
    "timestamps_failures = pd.to_datetime(df_powerSource_failures[:-1].timestamp_absolute, unit=\"ms\")\n",
    "timestamps_checkpoint = pd.to_datetime(df_powerSource_checkpoint[:-1].timestamp_absolute, unit=\"ms\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(timestamps, df_powerSource[:-1].energy_usage, label=\"no failures\", linewidth=2)\n",
    "ax.plot(timestamps_failures, df_powerSource_failures[:-1].energy_usage, label=\"failures\", linewidth=2)\n",
    "ax.plot(timestamps_checkpoint, df_powerSource_checkpoint[:-1].energy_usage, label=\"checkpoint\", linewidth=2)\n",
    "\n",
    "plt.title(\"Energy Usage during a workload (with and without failures)\", fontsize=15)\n",
    "plt.xlabel(\"Time\", fontsize=15)\n",
    "plt.ylabel(\"Energy Usage (kWh)\", fontsize=15)\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "myFmt = mdates.DateFormatter('%y-%m-%d %H:%M:%S')\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "runtime_normal = pd.to_timedelta(df_service.timestamp.max() - df_service.timestamp.min(), unit=\"ms\")\n",
    "runtime_failures = pd.to_timedelta(df_service_failures.timestamp.max() - df_service_failures.timestamp.min(), unit=\"ms\")\n",
    "runtime_checkpoint = pd.to_timedelta(df_service_checkpoint.timestamp.max() - df_service_checkpoint.timestamp.min(), unit=\"ms\")\n",
    "\n",
    "print(\"RUNTIME\")\n",
    "print(f\"The workload without failures took:        {runtime_normal}\")\n",
    "print(f\"The workload with failures took:           {runtime_failures}\")\n",
    "print(f\"The workload with checkpointing took:      {runtime_checkpoint}\")\n",
    "print(f\"Failures caused an increase of             {(runtime_failures - runtime_normal) / runtime_normal * 100:.2f}%\")\n",
    "print(f\"Checkpoints caused an increase of          {(runtime_checkpoint - runtime_normal) / runtime_normal * 100:.2f}%\\n\\n\")\n",
    "\n",
    "energy_normal = df_powerSource.energy_usage.sum()\n",
    "energy_failures = df_powerSource_failures.energy_usage.sum()\n",
    "energy_checkpoint = df_powerSource_checkpoint.energy_usage.sum()\n",
    "\n",
    "print(\"ENERGY\")\n",
    "print(f\"The workload without failures used         {energy_normal / 3_600_000:.0f} kWh\")\n",
    "print(f\"The workload with failures used            {energy_failures / 3_600_000:.0f} kWh\")\n",
    "print(f\"The workload with checkpointing used       {energy_checkpoint / 3_600_000:.0f} kWh\")\n",
    "print(f\"Failures caused an increase of             {(energy_failures - energy_normal) / energy_normal * 100:.2f} %\")\n",
    "print(f\"Checkpoints caused an increase of          {(energy_checkpoint - energy_normal) / energy_normal * 100:.2f} %\\n\\n\")\n",
    "\n",
    "carbon_normal = df_powerSource.carbon_emission.sum()\n",
    "carbon_failures = df_powerSource_failures.carbon_emission.sum()\n",
    "carbon_checkpoint = df_powerSource_checkpoint.carbon_emission.sum()\n",
    "\n",
    "print(\"CARBON\")\n",
    "print(f\"The workload without failures emitted      {carbon_normal:.0f} grams of carbon\")\n",
    "print(f\"The workload with failures emitted         {carbon_failures:.0f} grams of carbon\")\n",
    "print(f\"The workload with checkpointing emitted    {carbon_checkpoint:.0f} grams of carbon\")\n",
    "print(f\"Failures caused an increase of             {(carbon_failures - carbon_normal) / carbon_normal * 100:.2f} %\")\n",
    "print(f\"Checkpoints caused an increase of          {(carbon_checkpoint - carbon_normal) / carbon_normal * 100:.2f} %\\n\\n\")\n",
    "\n",
    "print(\"TASKS TERMINATED\")\n",
    "print(f\"With failures, the number of terminated tasks was:      {df_service_failures.tasks_terminated.sum()}\")\n",
    "print(f\"With checkpoints, the number of terminated tasks was:   {df_service_checkpoint.tasks_terminated.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
